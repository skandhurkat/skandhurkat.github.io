[
  {
    "authors": [
      "Skand Hurkat",
      "Jos√© F. Mart√≠nez"
    ],
    "categories": null,
    "content": "",
    "date": 1550620800,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1550620800,
    "objectID": "2bd9578ba1a4371f165145c380185fa8",
    "permalink": "https://skandhurkat.com/publication/hpca2019/",
    "publishdate": "2019-02-20T00:00:00Z",
    "relpermalink": "/publication/hpca2019/",
    "section": "publication",
    "summary": "We present Versatile Inference Processor (VIP), a highly programmable architecture for machine learning inference. VIP consists of 128 lightweight processing engines employing a vector processing paradigm, with a simple ISA and carefully chosen microarchitecture features. It is coupled with a modern, lightly customized, 3D-stacked memory system. Through detailed execution-driven simulations backed by RTL synthesis, we show that we can achieve online, real-time vision throughput (24 fps), at low power consumption, for both full- HD depth-from-stereo using belief propagation, and VGG-16 and VGG-19 deep neural networks (batch size of 1). Our RTL synthesis of a VIP processing engine in TSMC 28 nm technology, using a commercial standard-cell library supplied by ARM, results in 18 mm2 of silicon area and 3.5 W to 4.8 W of power consumption for all 128 VIP processing engines combined.",
    "tags": null,
    "title": "VIP: A Versatile Inference Processor",
    "type": "publication"
  },
  {
    "authors": null,
    "categories": null,
    "content": "CMake is cross-platform build and configuration system for C and C++ code, which also happens to be my favourite build system. In this post, I\u0026rsquo;ll present a quick tutorial on getting started with CMake.\nBut before we do, let\u0026rsquo;s start with an example of why other build systems such as Makefiles don\u0026rsquo;t necessarily cut it. Actually, Makefiles work fairly well, but writing Makefiles by hand is generally a pain in the neck. In a previous post, I described how to write Makefiles that take include file dependencies into account. If that post did not convince you that writing Makefiles is hard, let me show you another example of a terrible Makefile and hope that it scares you into never looking at a Makefile again. (This Makefile was actually part of a project that was handed over to me, I had to go over tonnes of other people\u0026rsquo;s code with comments written in non-ascii characters in a language that wasn\u0026rsquo;t English, so I clearly don\u0026rsquo;t have fond memories of this particular project.)\n## Compiler + Linker CC = g++ LD = g++ ## external .h files to bring in EXTRA_INCLUDES = CCFLAGS = -g -Wno-deprecated -I/opt/local/include/ -I/opt/local/include/opencv/ -I./GMM/ -I./MaxFlow/ -I./MeanShift/ LDFLAGS = -L/opt/local/lib #LIBS = -lcv -lcxcore -lhighgui -lcvaux #-lml # for macs LIBS = -lopencv_core -lopencv_highgui #-lcvaux -lml ## sources CCFILES_GMM = #### Some files CCFILES_MAXFLOW = #### Some more files CCFILES_MEANSHIFT = #### Holy shit, even more files CCFILES = $(CCFILES_GMM) $(CCFILES_MAXFLOW) $(CCFILES_MEANSHIFT) #### And even more files PROGFILE = main.cpp #### Seriously, how many files? ## object compilation PROGOBJS = $(CCFILES:.cpp=.o) $(PROGFILE:.cpp=.o) ## binary compilation PROG = BOO all: @echo \u0026quot;-\u0026gt; Building $(PROG) executable...\u0026quot; #\t@make .cpp.o @make $(PROG) .cpp.o: $(CC) $(CCFLAGS) $(EXTRA_INCLUDES) -c -o $@ $^ #$(PROGOBJS): #\t$(CC) $(CCFLAGS) $(EXTRA_INCLUDES) -c -o $@ $^ $(PROG): $(PROGOBJS) $(LD) $(LDFLAGS) $(LIBS) $(PROGOBJS) -o $(PROG) #$(PROGOBJS) $(LDFLAGS) $(LIBS) #$(PROG): $(CCFILES) #\t$(CC) $(CCFLAGS) $(EXTRA_INCLUDES) $(LDFLAGS) $(LIBS) -o $@ $^ clean: @echo \u0026quot;-\u0026gt; Cleaning...\u0026quot; rm -f $(PROG) $(PROGOBJS)  There\u0026rsquo;s just so much to hate about this Makefile. First, it is almost unreadable. Second, it does not account for any header file dependencies. In fact, if any of the header files change, there\u0026rsquo;s no way for the system to detect this at all, forcing the user to type in make clean, then make to make everything all over again. Third, given the sheer number of files, make will take minutes to run, which kills productivity. Fourth, if you look closely at the path, you will observe that the person writing this Makefile used a Mac with MacPorts and therefore is the kind of person who is just wrong. Fifth, this Makefile obviously won\u0026rsquo;t work for someone on Linux or Windows, even worse, it won\u0026rsquo;t work for another Mac user who used HomeBrew or compiled OpenCV from source. Sixth, some rules are just commented out, which indicates sloppy behaviour. Writing Makefiles like this just sucks. It sucks productivity and makes me think of slow and painful death.\nThe first thing I did when I was handed this project was to fix the build system, and CMake came to the rescue here. I\u0026rsquo;ll now show how I replaced this terrible Makefile with a clean, modular CMake build system that replicated the effects of the Makefile (without the bugs, of course üòÑ). Hopefully, the ease of setting up this CMake system will convince you to never write a Makefile again.\nAs a dive into CMake, let\u0026rsquo;s start with a simple \u0026lsquo;Hello World\u0026rsquo; program. The helloworld.cc file contains the following code:\n#include \u0026lt;iostream\u0026gt; int main(void) { std::cout \u0026lt;\u0026lt; \u0026quot;Hello World\u0026quot; \u0026lt;\u0026lt; std::endl; return 0; }  To compile this code, create a file called CMakeLists.txt:\nadd_executable(hello helloworld.cc)  Now, assuming that you are running on Linux or Mac OS terminal (something with bash), you can execute the following commands:\nmkdir build; cd build; cmake ..; make;  This code creates a directory called build inside the project directory. (An important point to note is that CMake always builds \u0026lsquo;out of source\u0026rsquo;, i.e. the build directory must be a distinct directory from the source directory.) It then switches to the build directory, executes CMake, which creates Makefiles (turns out that there is no escaping Makefiles after all\u0026hellip; üòÅ) that have all the necessary information. As these files are generated by a tool and not written by a person, you never have to read them, so Makefile readability is not a concern. Second, the Makefiles encode all the dependencies correctly, so there\u0026rsquo;s no reason to go through the hoops creating complex dependency files.\n\u0026lsquo;But what if I\u0026rsquo;m on Windows, or if I just am the kind of person who is always wrong and uses CodeBlocks, Xcode, or another IDE?\u0026rsquo; you ask. Well, hold on till the end, I\u0026rsquo;ll show you how CMake has you covered.\nAnyway, back to our hypothetical, but totally real project. The project has multiple source files in different directories.\n GMM contains files that implement a Gaussian mixture models code. MAXFLOW contains files for max-flow graph cuts. MEANSHIFT contains files for mean shift (cluster analysis).  All these codes are independent, so could be made into libraries. Of course, actually packaging this code as independent libraries is a terrible idea because the code isn\u0026rsquo;t well written in the first place. However, I don\u0026rsquo;t see too much harm in creating a static library that will probably never be distributed for code in each of these directories.\nWe can create a CMakeLists.txt file for each of these folders. The CMakeLists will contain code that looks like this:\nadd_library(# Library name STATIC # Need to create a static library, not a shared one. ### List of files )  Then the top-level CMakeLists.txt can include these directories through the simple command:\n# Tell CMake to look into subdirectories for CMakeLists.txt files. subdirs(GMM MAXFLOW MEANSHIFT) # Add these directories to the compiler search path for include files. # ${PROJECT_SOURCE_DIR} is the path to the source code in the project. include_directories( ${PROJECT_SOURCE_DIR}/GMM ${PROJECT_SOURCE_DIR}/MAXFLOW ${PROJECT_SOURCE_DIR}/MEANSHIFT )  Okay, but we also notice that the top-level program PROG needs OpenCV in order to compile. CMake provides a very easy method to search for dependencies. In the top-level CMakeLists.txt, we add\nfind_package(OpenCV REQUIRED)  When we compile the program PROG, we need to link it against the OpenCV libraries, as well as the static libraries in the GMM, MAXFLOW, MEANSHIFT folders.\ninclude_directories(${OpenCV_INCLUDE_DIRS}) add_executable(PROG PROG.cc) target_link_libraries(PROG ${OpenCV_LIBS} GMM MAXFLOW MEANSHIFT)  Putting it all together, the top-level CMakeLists.txt contains the following code:\n# Set a minimum version of CMake to be used, depending on the syntax of # the CMakeLists files. cmake_minimum_required(VERSION 2) # Give this project a cutsie name. project(PROG) # CMake subdirectories to include for these libraries. subdirs(GMM MAXFLOW MEANSHIFT) # Search for OpenCV. If OpenCV is found, the command sets the # ${OpenCV_INCLUDE_DIR} and ${OpenCV_LIBS} variables to appropriate # values. If OpenCV is not found, CMake will fail. find_package(OpenCV REQUIRED) # Tell the compiler to search these directories for include files. include_directories( ${PROJECT_SOURCE_DIR}/GMM ${PROJECT_SOURCE_DIR}/MAXFLOW ${PROJECT_SOURCE_DIR}/MEANSHIFT ${OpenCV_INCLUDE_DIR} ) # Finally, create the executable that we want. add_executable(PROG PROG.cc) # And link it against the required libraries. target_link_libraries(prog ${OpenCV_LIBS} GMM MAXFLOW MEANSHIFT)  And we\u0026rsquo;re done! Isn\u0026rsquo;t this CMakeLists.txt file so much more readable than the Makefile shown earlier? Not just that, this system will work with any OS, any compiler, any platform. Isn\u0026rsquo;t this great?\nOkay, okay, you\u0026rsquo;re the kind that likes to do things in an IDE. CMake has a concept of \u0026lsquo;generators\u0026rsquo;, which is just a fancy name for a backend. Still assuming that we are on MacOS or Linux, we simply execute the following commands.\nmkdir build; cd build; cmake -G \u0026quot;Xcode\u0026quot; .. # Or CodeBlocks, or whatever.  On Windows, execute the following commands on the command prompt (assuming that CMake is on your path) to create a Visual Studio 2015 project.\nmd build cd build cmake -G \u0026quot;Visual Studio 14\u0026quot; ..  Oh, did I mention that CMake also has a nice GUI that makes this entire process super easy? I won\u0026rsquo;t describe it here, I\u0026rsquo;ll just let you try it out for yourself.\nThe utility of CMake, however, doesn\u0026rsquo;t just end here. CMake has a very neat testing system called CTest, which is something that I have mentioned in an earlier post. This testing system interacts perfectly with a dashboard system called CDash, that can quickly allow people to view test results. CTest/CDash also has support for testing memory leaks and code coverage, so there. Finally, the last piece of the puzzle is CPack, which is a way of packaging software into installers.\nThe next post in the series will probably deal with advanced CMake configuration, including testing compiler versions, creating release and debug configurations. Until next time\u0026hellip;\n",
    "date": 1511472520,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1511472520,
    "objectID": "09613b8a3c4af71b2313bcb35040f660",
    "permalink": "https://skandhurkat.com/post/intro-to-cmake/",
    "publishdate": "2017-11-23T16:28:40-05:00",
    "relpermalink": "/post/intro-to-cmake/",
    "section": "post",
    "summary": "CMake is cross-platform build and configuration system for C and C++ code, which also happens to be my favourite build system. In this post, I\u0026rsquo;ll present a quick tutorial on getting started with CMake.\nBut before we do, let\u0026rsquo;s start with an example of why other build systems such as Makefiles don\u0026rsquo;t necessarily cut it. Actually, Makefiles work fairly well, but writing Makefiles by hand is generally a pain in the neck.",
    "tags": [
      "cmake",
      "programming"
    ],
    "title": "Introduction to CMake",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "I\u0026rsquo;ve seen way too many projects that supply a makefile that requires the user to run make clean and make every single time they make a change to some file. This is annoying and error prone, and the good news is that it can be easily solved with this one simple trick. Use a good build-generation system like CMake instead.\n\nStill here? Well, since you asked, I shall tell.\nThe trick is to have your compiler spit out a list of dependencies for each source file.\nTry it out right now. Start a terminal and go to any project that is sufficiently complex to have multiple include chains in a C or C++ source file (foo.cc). Then type in this command\n$\u0026gt; g++ -M foo.cc  The output is a list of all the files that are included by the source file, including standard library headers. If you look closer, the output is formatted so that it follows the same syntax as make for declaring dependencies. But we really don\u0026rsquo;t want to be bothered with system header files, so instead, we\u0026rsquo;ll use the g++ -MM.\nCool, isn\u0026rsquo;t it? This means that all we now need to do is to find a way to include this information in our Makefile, and have it automatically update any time the source files are changed. So here\u0026rsquo;s the recipe.\n# List of source files here. sources = foo.cc bar.cc # Add C++ flags, for example, if the code uses C++11 standards. CPPFLAGS = -std=c++11 # Recipe for making .d files from .cc files %.d: %.cc $(CC) -MM $(CPPFLAGS) -o $@ $\u0026lt; # Include the required .d files in the current Makefile. include $(sources:.cc=.d) # Recipe for making .o files. Here, I introduce a dependency on the # primary source c++ file. %.o: %.cc $(CC) -c $(CPPFLAGS) -o $@ $(\u0026lt;:.d=.cc)  Of course, there\u0026rsquo;s a bug in this recipe as well. Can you spot it? While you think about the bug, enjoy this picture of the gorge at Watkins Glen state park, which is under thirty miles from Cornell University.\n  View this post on Instagram         A post shared by Skand Hurkat (@skandhurkat) on Dec 1, 2016 at 7:35pm PST\n  The bug is that the .d files do not update if something changes in the include chain. However, the GNU Make manual has an interesting fix.\nTheir solution is to use sed to introduce the same dependency chain as for the .o file into the .d file. Here\u0026rsquo;s the updated Makefile with their recipe.\n# List of source files here. sources = foo.cc bar.cc # Add C++ flags, for example, if the code uses C++11 standards. CPPFLAGS = -std=c++11 # Recipe for making .d files from .cc files. This has been updated from # the GNU Make manual %.d: %.cc $(CC) -MM $(CPPFLAGS) $\u0026lt; \u0026gt; $@.$$$$; \\ sed 's,\\($*\\)\\.o[ :]*,\\1.o $@ : ,g' \u0026lt; $@.$$$$ \u0026gt; $@; \\ rm -f $@.$$$$ # Include the required .d files in the current Makefile. include $(sources:.cc=.d) # Recipe for making .o files. Here, I introduce a dependency on the # primary source c++ file. %.o: %.cc $(CC) -c $(CPPFLAGS) -o $@ $(\u0026lt;:.d=.cc)  Okay, I think that should work now. But it does beg the question, why are we even writing Makefiles by hand any more? Shouldn\u0026rsquo;t we just use a configuration system like CMake or GNU Autotools that can generate correct Makefiles?\n",
    "date": 1501710300,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1501710300,
    "objectID": "10d13208447fe0b5317dc9a45db252f4",
    "permalink": "https://skandhurkat.com/post/makefile-dependencies/",
    "publishdate": "2017-08-02T17:45:00-04:00",
    "relpermalink": "/post/makefile-dependencies/",
    "section": "post",
    "summary": "I\u0026rsquo;ve seen way too many projects that supply a makefile that requires the user to run make clean and make every single time they make a change to some file. This is annoying and error prone, and the good news is that it can be easily solved with this one simple trick. Use a good build-generation system like CMake instead.\n\n",
    "tags": [
      "makefiles",
      "programming"
    ],
    "title": "Handling dependencies in Makefiles",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "Today, my website displays a banner to join the fight for an open internet. My message to the FCC follows.\n\nThe FCC\u0026rsquo;s Open Internet Rules (net neutrality rules) are extremely important to me. I urge you to protect them.\nAs a person who is not an American citizen, I was led to believe (perhaps wrongly) that America as a capitalist society would value the very principles that allow a free market to function, namely a free flow of information and healthy competition. It seems that I was mistaken.\nThe proposed changes to the FCC rules negatively impact both these principles. The internet is the epitome of a free flow of information, however misunderstood it may be amongst those who make laws. A free and open internet is one where all data is treated equal, and ISPs and cable companies cannot distinguish between any kind of traffic. They should not be allowed to create fast lanes or zero-rate some form of traffic.\nThe creation of fast lanes or zero-rating some traffic while charging for other forms of traffic goes against the spirit of free competition in the market. It means that no one could create the next Google or NetFlix unless they can get ISPs on board with their plan and provide their traffic with the same high speed or low cost. It means that ISPs will have ultimate control, not just of their own market (which, as I shall discuss next is a monopoly), but also of other markets far removed from the internet.\nToday, almost everything runs on networked computers. Any system that must work across the width of a continent, or across multiple cities uses the internet in some way or the other to communicate information across. Even our phone lines are now increasingly routed over the internet, using VoIP technology. Our light bulbs and toaster ovens will soon use the internet as IoT becomes ubiquitous. So, support for net neutrality is not just a fight for the internet; absence of net neutrality and putting ISPs in position as gatekeeper would harm not just internet services, but could extend to the physical world as well, as the gap between physical and virtual worlds gets shorter and shorter. We must ask: what gives ISPs the right to decide what works and what doesn\u0026rsquo;t?\nThe sooner lawmakers realise this fundamental truth, the better. ISPs do not own the internet. They have not created the infrastructure for the internet. They have done nothing other than create a monopoly that taxes people to access a technology that was the product of taxpayer dollars and DARPA research.\nIndeed, ISPs are a monopoly in the USA. Right now, if I want internet access, I have only one option, Time Warner, now called Spectrum. If I lived in Pittsburgh, I would have only one option, Comcast. Monopolies are antithetical to the free market. But I don\u0026rsquo;t need to tell you that. A capitalist society should balk at the very idea of a monopoly. Yet, lawmakers seem to accept and deny the very existence of these monopolies in almost all facets of American life, rather than fulfil their role as regulators and regulate the industries with monopolies to serve the very people who elect them into office and not the industry lobbyists who pay them money.\nIf my country \u0026ndash; India \u0026ndash; could enact strong net neutrality protections at a point in its history when ubiquitous internet access was (and is) still not widespread throughout the country, I\u0026rsquo;m surprised that the country that gave the world the free internet is now so eager to destroy it to serve the interests of a small group of cable companies that had nothing to do with the creation of the internet, and through sheer coincidence ended up controlling everyone\u0026rsquo;s access to taxpayer funded technology.\n",
    "date": 1499864684,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1499864684,
    "objectID": "8112f922e19e15fd78eee86eb32e1fd5",
    "permalink": "https://skandhurkat.com/post/net-neutrality/",
    "publishdate": "2017-07-12T09:04:44-04:00",
    "relpermalink": "/post/net-neutrality/",
    "section": "post",
    "summary": "Today, my website displays a banner to join the fight for an open internet. My message to the FCC follows.\n\n",
    "tags": [
      "personal-opinions"
    ],
    "title": "Net Neutrality",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "Four pictures of the third of July fireworks in Ithaca to celebrate the fourth of July.\n\n   View this post on Instagram         A post shared by Skand Hurkat (@skandhurkat) on Jul 3, 2017 at 8:35pm PDT\n    View this post on Instagram         A post shared by Skand Hurkat (@skandhurkat) on Jul 3, 2017 at 8:43pm PDT\n    View this post on Instagram         A post shared by Skand Hurkat (@skandhurkat) on Jul 4, 2017 at 6:14am PDT\n    View this post on Instagram         A post shared by Skand Hurkat (@skandhurkat) on Jul 4, 2017 at 6:17am PDT\n \nOn second thought, because the fireworks were on the third of July, here are three bonus pictures.\n   View this post on Instagram         A post shared by Skand Hurkat (@skandhurkat) on Jul 3, 2017 at 8:35pm PDT\n    View this post on Instagram         A post shared by Skand Hurkat (@skandhurkat) on Jul 3, 2017 at 8:35pm PDT\n    View this post on Instagram         A post shared by Skand Hurkat (@skandhurkat) on Jul 4, 2017 at 6:17am PDT\n \n",
    "date": 1499178136,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1499178136,
    "objectID": "fd1463e06db823a16fe64f262699650f",
    "permalink": "https://skandhurkat.com/post/happy-fourth-july-2017/",
    "publishdate": "2017-07-04T10:22:16-04:00",
    "relpermalink": "/post/happy-fourth-july-2017/",
    "section": "post",
    "summary": "Four pictures of the third of July fireworks in Ithaca to celebrate the fourth of July.\n\n",
    "tags": [],
    "title": "Happy Fourth of July",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "I\u0026rsquo;m back from the 44th International Symposium on Computer Architecture, and this is a perfect time for me to summarise my thoughts on the conference.\nThe conference was in Toronto, which was a refreshing change for me to see correct spelling and sensible units for a change. Beyond that, the conference had a lot of interesting developments, and some that were not quite as interesting.\nFirst, let me address the 15-month elephant in the room. That\u0026rsquo;s Google\u0026rsquo;s Tensor processing unit (TPU) paper. I wasn\u0026rsquo;t impressed with the paper (although I am impressed by the engineering), although a lot of people seemed to be impressed by that paper. Indeed, only one other person I spoke with at the conference seemed to share my views on the matter. My criticism of the TPU paper is that it really gives little information. An application-specific integrated circuit (ASIC) will obviously have much less power/energy and much higher performance over a general purpose processor. The really interesting parts of the TPU would have been the Tensorflow-to-control-instructions compiler and driver. Unfortunately, these details still remain elusive. In fact, the whole paper describes (or fails to describe) technology that is over four years old and has already been replaced. In my opinion, I find more information in Google\u0026rsquo;s Project Zero blog than I did in the TPU paper and the associated talk.\nWhich brings me to the next bit. ISCA had a \u0026lsquo;Trends in Machine Learning\u0026rsquo; workshop, which I found as, if not more interesting than the main conference. There were some really cool demos, such as real-time neural networks running on embedded devices such as an iPhone and a Raspberry Pi, \u0026lsquo;Clinc\u0026rsquo;, which can process natural speech and respond to queries on your finances, DeepSpeech by Baidu, amongst others. The trend towards machine learning was apparent even through the main program, with multiple papers on accelerating neural networks.\nJust like any other conference, there were also some presentations that had me completely zoned out. There were some that felt just like a rehash of old ideas, and some which left me scratching my head.\nThe overarching theme of the keynotes and the panel discussion, however, were on the inevitable end of Moore\u0026rsquo;s law, with Mark Bohr from Intel claiming that the law was merely tired and shagged out after a long squawk, with plots showing that the number of transistors are indeed doubling as expected from Moore\u0026rsquo;s law. Partha Ranganathan from Google, on the other hand pointed out that the law had joined the choir invisible. Partha argued that this was indeed a fun time to be an architect, to talk to those annoying people working with software to co-design hardware and software with the point of unlocking more potential.\nIn fact, if there is one message that I would take away from the conference, it is that we computer architects have to fundamentally change the way we look at our job. For years, computer architects were perfectly happy using the extra transistors that the devices folks gave us to make faster computers, and the evil people working in software would take away this performance through even more bloated software. Now, the pipeline has dried up, the devices people are not able to give us faster and smaller transistors, and they definitely cannot give us more power-efficient transistors because Dennard scaling is almost certainly dead. As a result, we architects have to find ways to use these transistors more efficiently. This means talking to the software gremlins, understanding their evil algorithms and implementing them in beautiful silicon. The future is almost certainly in going green by reducing our power requirements and in grudgingly enabling the software people to unlock greater functionality, not by relying on faster computers, but by relying on custom, bespoke hardware that can run their algorithms in an efficient manner.\nHow such bespoke hardware should be deployed remains a challenge. We can almost certainly not sell chips for mobile phones with the area of a football field with billions of custom accelerators that are almost always turned off except for a few running a custom app. My opinion is that the best way to deploy these accelerators in the present moment is in datacentres, to provide them to users as a service. Google is already making great headway by allowing people to rent cloud machines with TPUs (I still dislike the TPU paper üòÑ) and to use TensorFlow to accelerate their workloads. I could envision cloud providers allowing people to time-multiplex multiple accelerators in some sort of mutual-fund or Massdrop like cloud service.\nOr maybe the future is in taking a step back and rethink our obsession with Von Neumann machines with variants of the five-stage pipeline and redraw our computers from scratch. I think it\u0026rsquo;s an exciting time to be an architect, and also scary. As a PhD candidate, I have to try really hard to look at the exciting bits and not the scary ones. üòÑ\n",
    "date": 1498771723,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1498771723,
    "objectID": "5e08b8ad836a3a0d4d62839b67726267",
    "permalink": "https://skandhurkat.com/post/thoughts-isca-2017/",
    "publishdate": "2017-06-29T17:28:43-04:00",
    "relpermalink": "/post/thoughts-isca-2017/",
    "section": "post",
    "summary": "I\u0026rsquo;m back from the 44th International Symposium on Computer Architecture, and this is a perfect time for me to summarise my thoughts on the conference.\nThe conference was in Toronto, which was a refreshing change for me to see correct spelling and sensible units for a change. Beyond that, the conference had a lot of interesting developments, and some that were not quite as interesting.\nFirst, let me address the 15-month elephant in the room.",
    "tags": [
      "isca"
    ],
    "title": "My thoughts on ISCA 2017",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "I\u0026rsquo;ve seen a lot of people (I\u0026rsquo;m looking at you Daniel Lemire) praise newer languages like Go, which makes me sad as a C++ programmer. Well, that\u0026rsquo;s until I realise that most of the features touted could be easily incorporated in C++ with just a bit of elbow grease. In this post, I\u0026rsquo;ll show how to add an automated testing system using CTest.\n\nI love CMake, it allows me to write cross-platform code and be confident that the build system would work across a choice of compilers, IDEs, and operating systems. When writing large projects, however, it is imperative to have a series of tests that could indicate regression bugs whenever new features are added. While I love the testing support built into languages such as Java, writing tests is not all that hard in C++ either. For example, I could simply write a function that mimics a unit test and prints out either \u0026ldquo;Test passed\u0026rdquo; or \u0026ldquo;Test failed\u0026rdquo; depending on the result of the test. All I now need is a way to automatically run these tests.\nThis is where CTest comes into the picture. I\u0026rsquo;m assuming here that you are using CMake as your build system. If you are not, then you are clearly wrong, a terrible person, and the type who would probably not write tests anyway. So stop reading this post. üòÑ\nOkay, back from that little digression.\nMy setup for testing is thus: I am building a library that defines all the required functions. Each class/function/API must be tested, so I write tests for each of these. The tests are written so that they would indicate success using the keyword \u0026ldquo;Test passed\u0026rdquo; or failure using the keyword \u0026ldquo;Test failed\u0026rdquo;. I put all the tests in a separate tests folder. Within the tests folder, I write my CMakeLists.txt thus.\nset(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/tests) set(CTEST_BINARY_DIRECTORY ${PROJECT_BINARY_DIR}/tests) file(GLOB files \u0026quot;test_*.cc\u0026quot;) foreach(file ${files}) string(REGEX REPLACE \u0026quot;(^.*/|\\\\.[^.]*$)\u0026quot; \u0026quot;\u0026quot; file_without_ext ${file}) add_executable(${file_without_ext} ${file}) target_link_libraries(${file_without_ext} ${PROJECT_LIBS}) add_test(${file_without_ext} ${file_without_ext}) set_tests_properties(${file_without_ext} PROPERTIES PASS_REGULAR_EXPRESSION \u0026quot;Test passed\u0026quot;) set_tests_properties(${file_without_ext} PROPERTIES FAIL_REGULAR_EXPRESSION \u0026quot;(Exception|Test failed)\u0026quot;) set_tests_properties(${file_without_ext} PROPERTIES TIMEOUT 120) endforeach()  This is a really simple script. It loops over all files in the tests folder that match the pattern test_*.cc, i.e.\\ C++ files that start with the test_ prefix. It generates the executable name by stripping out all the leading directories until the path, while also stripping out the extension for the file. It compiles the file and links it against the project libraries. Finally, it tells CMake/CTest that the compiled binary is a test which on passing would have in its output the text \u0026ldquo;Test passed\u0026rdquo;, and on failure could have the text \u0026ldquo;Test failed\u0026rdquo; or \u0026ldquo;Exception\u0026rdquo;. Finally, I add a timeout of 120 seconds, or two minutes. If the test runs longer than this time, it will be automatically terminated and marked as a failure. This is not ideal \u0026ndash; some of my tests run for just seconds, while the longest test could run for just under a minute. However, the purpose of the timeout here is not to detect performance bugs, rather to prevent the machine running the tests from fritzing out because of a bug that results in the tests doing something really crazy.\nOh, and in order to enable testing, I simply change the CMakeLists.txt on the very top level of the project (the one that defines project name, version, etc.) to include this one extra line.\ninclude(CTest)  Simple, isn\u0026rsquo;t it? Now why would you go running to Go (see what I did there? üòÑ), when C++ with CMake offers cross platform builds, testing infrastructure, and parallel execution with memory consistency models?\nIn the next posts on the series, I might just describe how to test for memory leaks using Valgrind/MemCheck or how to test test-coverage (alliteration FTW) using gcov. Stay tuned.\n",
    "date": 1498143943,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1498143943,
    "objectID": "19ce56fa614016dc4eaca0bcca38b702",
    "permalink": "https://skandhurkat.com/post/intro-to-ctest/",
    "publishdate": "2017-06-22T11:05:43-04:00",
    "relpermalink": "/post/intro-to-ctest/",
    "section": "post",
    "summary": "I\u0026rsquo;ve seen a lot of people (I\u0026rsquo;m looking at you Daniel Lemire) praise newer languages like Go, which makes me sad as a C++ programmer. Well, that\u0026rsquo;s until I realise that most of the features touted could be easily incorporated in C++ with just a bit of elbow grease. In this post, I\u0026rsquo;ll show how to add an automated testing system using CTest.\n\n",
    "tags": [
      "cmake",
      "programming",
      "c++"
    ],
    "title": "An Introduction to CTest",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "Vim is my favourite text editor, because it is minimalist while also being insanely configurable. While I may describe my ideal Vim setup sometime in the future, I do want to share some tips and tricks that make Vim so damn efficient for certain tasks. Today, I shall focus on macros.\nHere\u0026rsquo;s the key idea to keep in mind when discussing about Vim. Vim is not just an editor, it is a text manipulation program. Inserting text is just one of the many tasks that Vim can accomplish. A macro is a small Vim program that the user can record in order to make text editing more efficient.\nConsider a simple use case. Let\u0026rsquo;s say that I have a CSV file that looks like this\nNew York,NY,USA Seattle,WA,USA Mumbai,MH,India  Let\u0026rsquo;s say that I want to add another column at the beginning of each line that is sequentially numbered, like so\n1,New York,NY,USA 2,Seattle,WA,USA 3,Mumbai,MH,India  I could go down each line and manually add each column, but there\u0026rsquo;s a better way. The solution is to program a Vim macro that does the following tasks in order.\n Assuming that the first column of a certain line is the required number for this line (row), we copy the first column to the next line.\n Increment the line number by one so that it is one more than the previous line (row).\n  So, I edit the first line, in order to manually add the new column to the first line.\n1,New York,NY,USA Seattle,WA,USA Mumbai,MH,India  Next, I ensure that I\u0026rsquo;m not executing any Vim command. I can verify this by pressing ESC a couple of times. Then I press q followed by a letter to identify this macro, e.g.: a. The macro then consists of the following keystrokes\n 0: Move to the beginning of the line. v: Visual mode. /,: Find the first , to match. y: Yank (copy) the highlighted text. j0: Go to the beginning of the next line. P: Paste the copied text. At this point, the cursor should be on the ,. h: Move the cursor one position to the left so that it is on the last digit of the number. Ctrla: Increment the number. q: Stop recording the macro.  Now, pressing @a would perform the nine steps outlined with just two key-presses. Let\u0026rsquo;s say the file had 50 more lines to be edited. In this case, I could use another powerful trick that Vim offers \u0026ndash; the ability to repeat a command multiple times. So, ensure that I\u0026rsquo;m out of executing any command, I could type in a number followed by a command that would execute the specified command a certain number of times. So, pressing 50@a would execute the macro a fifty times, which is an insanely powerful technique for modifying multiple lines with a single command.\nThat\u0026rsquo;s all for Vim macros for today. I hope you found this post useful. Macros were one of the things I learnt relatively late as a Vim user, but now that I do know about this trick, I use it all the time, and it has saved me countless hours and a lot of headache when modifying code or text. Until the next time, when I take some time to write some more about cool Vim tricks and tips.\n",
    "date": 1496604789,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1496604789,
    "objectID": "4061c9399d7d352be4e59c55c345affc",
    "permalink": "https://skandhurkat.com/post/intro-to-vim-macros/",
    "publishdate": "2017-06-04T15:33:09-04:00",
    "relpermalink": "/post/intro-to-vim-macros/",
    "section": "post",
    "summary": "Vim is my favourite text editor, because it is minimalist while also being insanely configurable. While I may describe my ideal Vim setup sometime in the future, I do want to share some tips and tricks that make Vim so damn efficient for certain tasks. Today, I shall focus on macros.\nHere\u0026rsquo;s the key idea to keep in mind when discussing about Vim. Vim is not just an editor, it is a text manipulation program.",
    "tags": [
      "vim"
    ],
    "title": "An Introduction to Vim Macros",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "Some days ago, I became aware of a bug in GCC that has apparently existed since 2015. As this is a bug that deals with memory leaks, it is fairly serious.\nIn this case, I really don\u0026rsquo;t want to re-post the entire content of the source, so I will only list my experiments with reproducing the bug once I became aware of it.\nC++ assumes that if a constructor fails, then no memory is allocated for the object at all. This means that if a nested object was constructed, C++ will implicitly call the destructor for that object so that the programmer does not have to worry about partially constructed states. This bug in GCC exists because under certain circumstances, GCC fails this assumption.\nLet\u0026rsquo;s start off with a simple example trying to reproduce this bug.\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;exception\u0026gt; class inner { private: char* array; public: inner() : array(new char[128]) { std::cout \u0026lt;\u0026lt; \u0026quot;inner constructed\u0026quot; \u0026lt;\u0026lt; std::endl; } inner(const inner\u0026amp; other) : array(other.array) { std::cout \u0026lt;\u0026lt; \u0026quot;inner copy constructed\u0026quot; \u0026lt;\u0026lt; std::endl; } ~inner() { delete[] array; std::cout \u0026lt;\u0026lt; \u0026quot;inner destructed\u0026quot; \u0026lt;\u0026lt; std::endl; } }; inner make_inner_1() {return inner();} inner make_inner_2() {throw std::runtime_error(\u0026quot;exception\u0026quot;);} struct outer { inner i1; inner i2; }; int main(void) { try { outer o{make_inner_1(), make_inner_2()}; } catch (const std::exception\u0026amp; e) { std::cout \u0026lt;\u0026lt; \u0026quot;Abort after exception \u0026quot; \u0026lt;\u0026lt; e.what() \u0026lt;\u0026lt; std::endl; } return 0; }  Upon compiling and running this code, the output is\ninner constructed inner destructed Abort after exception exception  This behaviour is as is expected given C++ standards. The constructor for object outer tries to construct i1, succeeds, then tries to construct i2, fails, and destructs i1.\nWell, what if we try to create a nameless temporary object?\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;exception\u0026gt; class inner { private: char* array; public: inner() : array(new char[128]) { std::cout \u0026lt;\u0026lt; \u0026quot;inner constructed\u0026quot; \u0026lt;\u0026lt; std::endl; } inner(const inner\u0026amp; other) : array(other.array) { std::cout \u0026lt;\u0026lt; \u0026quot;inner copy constructed\u0026quot; \u0026lt;\u0026lt; std::endl; } ~inner() { delete[] array; std::cout \u0026lt;\u0026lt; \u0026quot;inner destructed\u0026quot; \u0026lt;\u0026lt; std::endl; } }; inner make_inner_1() {return inner();} inner make_inner_2() {throw std::runtime_error(\u0026quot;exception\u0026quot;);} struct outer { inner i1; inner i2; }; struct foo { outer o; }; int main(void) { try { foo{outer({make_inner_1(), make_inner_2()})}; } catch (const std::exception\u0026amp; e) { std::cout \u0026lt;\u0026lt; \u0026quot;Abort after exception \u0026quot; \u0026lt;\u0026lt; e.what() \u0026lt;\u0026lt; std::endl; } return 0; }  In this case, the output is\ninner constructed Abort after exception exception  This introduces a memory leak, because while i1 is constructed, it is never destroyed. If I compile using clang (version 3.8.0-2ubuntu4), the expected output is still correct.\ninner constructed inner destructed Abort after exception exception  The fact that this bug has remained unresolved for over two years is surprising. At this point, I\u0026rsquo;m tempted to point out a paper by Ken Thompson, Reflections on trusting trust, which points out that an untrustworthy compiler could introduce a bug in all or a few select programs that it compiles. Moreover, the compiler could be engineered to introduce a bug in its own binary, so compiling the compiler from source wouldn\u0026rsquo;t help either.\nThis is a subtle bug that took me some time to reproduce. However, I can easily imagine how frustrating it could be if this bug were to manifest in a larger program that would show up through the use of memcheck or a similar tool. In the meanwhile, we cannot do much except wait for a patch. This bug hasn\u0026rsquo;t shown up yet in code I\u0026rsquo;ve written, but I can always switch my build system to clang/Makefiles (the joys of writing platform-independent code with CMake as a build system).\n",
    "date": 1495551102,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1495551102,
    "objectID": "6ae05ed918eb2f5b39acc76c37d95395",
    "permalink": "https://skandhurkat.com/post/gcc-constructor-bug/",
    "publishdate": "2017-05-23T10:51:42-04:00",
    "relpermalink": "/post/gcc-constructor-bug/",
    "section": "post",
    "summary": "Some days ago, I became aware of a bug in GCC that has apparently existed since 2015. As this is a bug that deals with memory leaks, it is fairly serious.\nIn this case, I really don\u0026rsquo;t want to re-post the entire content of the source, so I will only list my experiments with reproducing the bug once I became aware of it.\nC++ assumes that if a constructor fails, then no memory is allocated for the object at all.",
    "tags": [
      "programming",
      "c++"
    ],
    "title": "A bug in how GCC handles constructors",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "Mark Buckler and I use Docker a lot. Sometimes, we need X11 forwarding to work over SSH. Here\u0026rsquo;s a summary of the steps involved in case you\u0026rsquo;re curious.\n\nEdit the Docker file Add these lines:\nRUN apt-get update RUN apt-get install -qqy x11-apps  This installs the necessary X packages on the Docker image.\nRun this once per SSH session XAUTH=$HOME/.Xauthority touch $XAUTH  This code sets the path to the .Xauthority file. When using SSH with X11 forwarding, the .Xauthority file is automatically created in the user\u0026rsquo;s $HOME folder. If not using X11 forwarding (for instance, when operating locally on the machine), touch will create the .Xauthority file.\nStart the Docker image docker run --tty --interactive --network=host --env DISPLAY=$DISPLAY --volume $XAUTH:/root/.Xauthority \u0026lt;dockerimagename\u0026gt;  This starts a Docker image with a TTY, in interactive mode, using the host\u0026rsquo;s networking stack (i.e. the container and the host are identical in their network usage, so port XX in the container is port XX on the host. It also exports the $DISPLAY environment variable to Docker, and binds the .Xauthority file (on the host) to the root user within the container.\nThat\u0026rsquo;s all folks! I suspect that these instructions will also be cross-posted on Mark\u0026rsquo;s website. You should absolutely follow his blog.\n",
    "date": 1493151707,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1493151707,
    "objectID": "819b1df382e1eba96adbd43b36ff47aa",
    "permalink": "https://skandhurkat.com/post/x-forwarding-on-docker/",
    "publishdate": "2017-04-25T16:21:47-04:00",
    "relpermalink": "/post/x-forwarding-on-docker/",
    "section": "post",
    "summary": "Mark Buckler and I use Docker a lot. Sometimes, we need X11 forwarding to work over SSH. Here\u0026rsquo;s a summary of the steps involved in case you\u0026rsquo;re curious.\n\n",
    "tags": [
      "docker",
      "linux"
    ],
    "title": "X forwarding on Docker",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "With the \u0026lt;random\u0026gt; header in C++11 onwards, there really is no reason to use std::rand() to generate random numbers. In fact, using std::rand() could be really harmful.\n\nstd::rand() generates random numbers using a linear congruential engine. This is a simple formula \u0026ndash; $x_{n+1} = (a x_n + b) \\mod c$. The trouble with such a simple engine is that the numbers generated have very low entropy.\nBut I really don\u0026rsquo;t care about entropy, it\u0026rsquo;s not like I\u0026rsquo;m writing a cryptography application, you say? Well, let me tell you a cautionary tale about how using std::rand() sent me on a wild goose chase for over a week trying to hunt down a bug miles from where it really was.\nHere\u0026rsquo;s the story. I was writing a piece of code that was supposed to return events at a random time, somewhere between x and y cycles in the future. I did not really care that the random numbers generated were good, I just wanted something quick and dirty. So, I ended up using code that looked like this:\ntime_type new_event_time = get_current_time() + (std::rand() % (y - x)) + x;  Looks good, and behaved reasonably well for almost all the test cases I threw at the problem. However, one specific test case was really sensitive to the order in which events returned, but only if the number of pending events was larger than a certain number z. This test started to fail, as is to be expected. However, the failures happened a great deal into the future, which suggested some weird errors in bookkeeping that would manifest after a certain number of insertions and deletions. This forced me to go on a bug-hunting spree that revealed absolutely nothing. Then, I discovered a talk by STL, which is the inspiration for this post as well. In this talk, STL shows how bad the linear congruential engine in the std::rand() function really is.\nSo, I simply replaced the random number generation code. The std::srand() equivalent code is\nstd::random_device rd; std::mt19937 mt(rd()); std::uniform_int_distribution dist(x, y);  and the equivalent code for\ntime_type new_event_time = get_current_time() + (std::rand() % (y - x)) + x;  simplifies to\ntime_type new_event_time = get_current_time() + dist(mt);  Simple, isn\u0026rsquo;t it? With this new piece of code, my test began to fail much earlier, which prompted me to discover the real source of the bug.\nI was intimidated by the new C++11 random number generation library because it requires the creation of a random device, a pseudo-random engine and a distribution, which is three more than a call to std::rand(). However, the low quality of std::rand() makes it unsuitable for even the simplest tasks requiring random numbers. My advice is, don\u0026rsquo;t ever use std::rand() and think you can get away with it.\n",
    "date": 1492386651,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1492386651,
    "objectID": "1283a32a23bc2bc581cfc8383d42ed17",
    "permalink": "https://skandhurkat.com/post/psa-dont-use-rand/",
    "publishdate": "2017-04-16T19:50:51-04:00",
    "relpermalink": "/post/psa-dont-use-rand/",
    "section": "post",
    "summary": "With the \u0026lt;random\u0026gt; header in C++11 onwards, there really is no reason to use std::rand() to generate random numbers. In fact, using std::rand() could be really harmful.\n\n",
    "tags": [
      "programming",
      "c++"
    ],
    "title": "PSA: Don't use std::rand()",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "On Monday and Tuesday, we graduate students at Cornell will be voting on whether or not we want to unionise. Actually, scratch that, only graduate students who hold a TA, RA, or GRA appointment can unionise.\nThis is a shitty arrangement, and I will be voting against it.\nFor those of you who are not aware of how graduate school works at Cornell, you could be on one of many appointments.\n Fellowship A graduate student on a fellowship gets a stipend and tuition paid without associated teaching or research opportunities. Graduate students on a fellowship typically work towards their own theses, but will be excluded from the union\n Graduate research assistantships A GRA gives a graduate student stipend and tuition without teaching responsibilities. However, this money comes out of a specific project grant, and the students typically work on their own theses. Students on GRAs magically qualify to join the union, whereas there is virtually no difference between a GRA and a fellowship for the most part.\n Research assistantships An RA appointment provides stipend and tuition without teaching responsibilities, but with research responsibilities. Students on an RA appointment must work on projects that may not be their own theses. RAs qualify to form a union\n Teaching assistantships A TA appointment provides stipend and tuition with teaching responsibilities. TA appointments qualify to form a union.\n  This breakdown shows that the union organisation is already on tenuous ground, as students could move in and out of these various appointments on a semester-by-semester basis. Receiving and losing union protection every four months is like not receiving any protection at all. If the union truly believes that students are exploited by professors (which I must categorically state, I am not. I love my job, I love my lab, and I love my advisor), then they are leaving a gaping loophole wherein a professor could simply move a student out of one of the protected appointments to an unprotected appointment, then crap all over them.\nBut that\u0026rsquo;s not the primary reason I am opposed to the union. At every stage in campaigning for our votes, the union has been shifty and deceitful, sometimes resorting to outright lies that could be easily disproved. For instance, the union always brought up a case of a student in the department of chemical engineering who lost his appointment at a lab following an accident that allegedly left him unable to control his right hand for a couple of years. The union\u0026rsquo;s position was, well, if only there were a union to fight for workers\u0026rsquo; compensation, that student would still be at Cornell. Here\u0026rsquo;s the lie \u0026ndash; students on a GRA, RA, or TA are already protected by workers\u0026rsquo; compensation. That particular student was on a fellowship, and this underscores my point about the shittiness of students receiving different protections for the same work based on an arbitrary status. In this student\u0026rsquo;s case, the union would have simply walked away with a \u0026lsquo;Not my problem\u0026rsquo; statement.\nNow, I am a fan of collective bargaining. I believe that we students may still get a lot done through collective bargaining, but the union in its current form is not the way to do so. For starters, the union will not cover all graduate students, and I abhor the idea of leaving some fellow students in virtually the same position as myself without the protections of collective bargaining simply based on a technicality. But this is ignoring the fact that Cornell has an organisation that could achieve collective bargaining for graduate students, the Graduate and Professional Students Assembly (GPSA), which has successfully managed to achieve many things in Cornell, such as negotiating our wages to be raised at 2% a year, above the current inflation rate of 1%. The last president, Elizabeth Garrett also increased graduate student salaries across the board, making RA and GRA salaries the same as TA salaries. Now, asking us to put $500\u0026ndash;1000 towards union dues is stupid, considering that we make $30,000 a year before state and federal income taxes. What\u0026rsquo;s also crazy is that the union apparently will have a two-tiered structure, wherein we may be expected to pay more in order to have a vote (a voice) in the union. This is taxation without representation, and I will always vote against such an arrangement on a matter of principle.\nAdditionally, the union seems like a power grab by a bunch of sloppy students. The entire process was undemocratic to say the least, and seemed just like a page out of Catch-22. Sign the union authorisation card. Well, what am I signing up for? Come to our meetings and find out. Cool, how do I do that? You must sign the card to come to our meetings. The process was secret, with no voice from graduate students who may have been on the fence, and with further amendments to artificially inflate the voices of some students. Take, for instance, this amendment, which grants equal voice on the negotiating committee to students from the six jurisdictions, whereas the number of students in these jurisdictions are by no means the same.\nFurther, some of the provisions on the Cornell GSU constitution defy logic, and show that they were probably written by monkeys on typewriters. See, for instance, Section IV.A.A2, which states that any grievance against any member of the union will be shared across all members of the union. I do not see this as helpful. While transparency is good, it should be balanced with privacy. Publishing a list of grievances without verification of the claims is irresponsible, and promotes a lynch-mob mentality which will make the environment at Cornell a lot more hostile and toxic. I realise that I am not providing evidence to support this claim \u0026ndash; the fact is that I am basing this off an existing incident which I do not wish to publicise, firstly, because I heard this account second-hand, and in order to protect the privacy of the persons that may have been involved.\nLastly, the process of getting rid of a dysfunctional union will be much, much harder than the process of voting one in. So, to my friends who say that they want to \u0026lsquo;try it out\u0026rsquo;, let me remind them that this is the equivalent of \u0026lsquo;trying out\u0026rsquo; arson. You will probably get burned, and you\u0026rsquo;ll be leaving a whole future generation of graduate students with a mess that they\u0026rsquo;ll have to deal with for the entirety of their graduate student life.\nTo conclude, the union in its current form is a juvenile power grab without a stated objective, a mission, or a plan to achieve its objectives. Voting for the union is the equivalent of voting for Donald Trump, and I hope that my friends can now see how that\u0026rsquo;s working out for the USA. Do not, do not vote for the union. Vote against it, let them come up with something better instead of an empty promise for something better. We can always vote for a better union next year. But we cannot vote out a shitty union once it\u0026rsquo;s formed.\n",
    "date": 1490478979,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1490478979,
    "objectID": "20038ae8a121189cfca8fcce296c9d08",
    "permalink": "https://skandhurkat.com/post/cgsu-at-what-cost/",
    "publishdate": "2017-03-25T17:56:19-04:00",
    "relpermalink": "/post/cgsu-at-what-cost/",
    "section": "post",
    "summary": "On Monday and Tuesday, we graduate students at Cornell will be voting on whether or not we want to unionise. Actually, scratch that, only graduate students who hold a TA, RA, or GRA appointment can unionise.\nThis is a shitty arrangement, and I will be voting against it.\nFor those of you who are not aware of how graduate school works at Cornell, you could be on one of many appointments.",
    "tags": [
      "personal-opinions"
    ],
    "title": "Cornell Graduate Students United: At what cost?",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "Winter is a seriously underrated season at Cornell. These seven pictures will show you just how beautiful the campus gets during a thick snow shower.\n\n   View this post on Instagram        Beebe lake during a snowstorm Beebe Lake, Cornell University. Taken during a snowstorm #ithaca #upstateny #waterfall #snow #snowstorm #cornell #cornelluniversity #winter #winterwonderland #stella #winterstormstella\nA post shared by Skand Hurkat (@skandhurkat) on Mar 14, 2017 at 1:25pm PDT\n    View this post on Instagram        Klarman Hall A police car drives past Klarman Hall during a snow storm, Cornell University #ithaca #upstateny #snow #snowstorm #winterwonderland #winter #cornell #cornelluniversity #nofilter #pentax #lightroom #stella #winterstormstella\nA post shared by Skand Hurkat (@skandhurkat) on Mar 14, 2017 at 1:26pm PDT\n    View this post on Instagram        Sharing an Umbrella Friends share an umbrella during a snowstorm, Cornell University. This storm was so severe that Cornell, which usually remains open through all sorts of inclement weather, was forced to close. #ithaca #upstateny #snow #snowstorm #winterwonderland #winter #stella #winterstormstella #cornell #cornelluniversity #nofilter #pentax #lightroom\nA post shared by Skand Hurkat (@skandhurkat) on Mar 14, 2017 at 1:27pm PDT\n    View this post on Instagram        Stopping by woods on a snowy evening Students leaving class, as Cornell University shuts down due to a snowstorm #ithaca #upstateny #snow #snowstorm #winterwonderland #winter #cornell #cornelluniversity #nofilter #pentax #lightroom #stella #winterstormstella\nA post shared by Skand Hurkat (@skandhurkat) on Mar 14, 2017 at 1:28pm PDT\n    View this post on Instagram        Stella TCAT buses were running as long as it remained safe during the winter storm Stella. Here, a TCAT bus drives past a pedestrian on Thurston Avenue, Cornell University. #ithaca, #cornell, #cornelluniversity, #snow, #snowstorm, #winter, #winterwonderland, #stella, #winterstormstella, #gradschoolstories, #ivyleaguecollective, #ivyleague, #nofilter\nA post shared by Skand Hurkat (@skandhurkat) on Mar 15, 2017 at 9:13am PDT\n    View this post on Instagram        Spot the road A snowplough clears the road during winter storm Stella, Cornell University. #ithaca, #cornell, #cornelluniversity, #snow, #snowstorm, #winter, #winterwonderland, #stella, #winterstormstella, #snowplow, #snowplough, #gradschoolstories, #ivyleaguecollective, #ivyleague, #nofilter\nA post shared by Skand Hurkat (@skandhurkat) on Mar 19, 2017 at 8:42am PDT\n    View this post on Instagram        Walk the dog A woman walks her dog during winter storm Stella. Thurston Ave bridge, Cornell University. Good thing the dog is wearing a thick fur coat. #ithaca, #cornell, #cornelluniversity, #snow, #snowstorm, #winter, #winterwonderland, #stella, #winterstormstella, #dog, #gradschoolstories, #ivyleaguecollective, #ivyleague, #nofilter\nA post shared by Skand Hurkat (@skandhurkat) on Mar 19, 2017 at 8:42am PDT\n \n",
    "date": 1489528909,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1489528909,
    "objectID": "333c36452edc9c2d040d378b1d8bae9a",
    "permalink": "https://skandhurkat.com/post/seven-pictures-storm-stella/",
    "publishdate": "2017-03-14T18:01:49-04:00",
    "relpermalink": "/post/seven-pictures-storm-stella/",
    "section": "post",
    "summary": "Winter is a seriously underrated season at Cornell. These seven pictures will show you just how beautiful the campus gets during a thick snow shower.\n\n",
    "tags": [
      "photographs"
    ],
    "title": "Seven pictures from the storm Stella that show the beauty of the Ithaca winter",
    "type": "post"
  },
  {
    "authors": [
      "Skand Hurkat",
      "Jungwook Choi",
      "Eriko Nurvitadhi",
      "Jos√© F. Mart√≠nez",
      "Rob A. Rutenbar"
    ],
    "categories": null,
    "content": "",
    "date": 1441215939,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1441215939,
    "objectID": "1e720f33d7b359ca1051a406d5047f02",
    "permalink": "https://skandhurkat.com/publication/fpl2015/",
    "publishdate": "2015-09-02T13:45:39-04:00",
    "relpermalink": "/publication/fpl2015/",
    "section": "publication",
    "summary": "Maximum a posteriori probability (MAP) inference on Markov random fields (MRF) is the basis of many computer vision applications. Sequential tree-reweighted belief propagation (TRW-S) has been shown to provide very good inference quality and strong convergence properties. However, software TRW-S solvers are slow due to the algorithm's high computational requirements. A state-of-the-art FPGA implementation has been developed recently, which delivers substantial speedup over software. In this paper, we improve upon the TRW-S algorithm by using a multi-level hierarchical MRF formulation. We demonstrate the benefits of Hierarchical-TRW-S over TRW-S, and incorporate the proposed improvements on a Convey HC-1 CPU-FPGA hybrid platform. Results using four Middlebury stereo vision benchmarks show a 21% to 53% reduction in inference time compared with the state-of-the-art TRW-S FPGA implementation. To the best of our knowledge, this is the fastest hardware implementation of TRW-S reported so far.",
    "tags": null,
    "title": "Fast hierarchical implementation of sequential tree-reweighted belief propagation for probabilistic inference",
    "type": "publication"
  },
  {
    "authors": [
      "Eriko Nurvitadhi",
      "Gabriel Weisz",
      "Yu Wang",
      "Skand Hurkat",
      "Marie Nguyen",
      "James C. Hoe",
      "Jos√© F. Mart√≠nez",
      "Carlos Guestrin"
    ],
    "categories": null,
    "content": "",
    "date": 1399831002,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1399831002,
    "objectID": "a3492f2ec25391cf1ca8b166e173d493",
    "permalink": "https://skandhurkat.com/publication/fccm2014/",
    "publishdate": "2014-05-11T13:56:42-04:00",
    "relpermalink": "/publication/fccm2014/",
    "section": "publication",
    "summary": "Vertex-centric graph computations are widely used in many machine learning and data mining applications that operate on graph data structures. This paper presents GraphGen, a vertex-centric framework that targets FPGA for hardware acceleration of graph computations. GraphGen accepts a vertex-centric graph specification and automatically compiles it onto an application-specific synthesized graph processor and memory system for the target FPGA platform. We report design case studies using GraphGen to implement stereo matching and handwriting recognition graph applications on Terasic DE4 and Xilinx ML605 FPGA boards. Results show up to 14.6 and 2.9 speedups over software on Intel Core i7 CPU for the two applications, respectively.",
    "tags": null,
    "title": "GraphGen: An FPGA framework for vertex-centric graph computation",
    "type": "publication"
  },
  {
    "authors": [
      "Skand Hurkat"
    ],
    "categories": null,
    "content": "This was a course project for CS6782 \u0026ndash; Probabilistic graphical models. In this project, I study the application of two techniques to the belief-propagation based decoding of LDPC codes, and compare these techniques against a traditional sum-product LDPC decoder. The first technique, min-sum decoding was implemented in the 1980s by Tanner as a method to reduce computational complexity of sum-product decoding. The other technique, residual belief propagation optimises the order in which message updates are scheduled in an informed manner, leading to faster and better convergence. The two methods are combined and experimentally evaluated.\n",
    "date": 1386989605,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1386989605,
    "objectID": "76896a547feffca35d32366868782e54",
    "permalink": "https://skandhurkat.com/project/ldpc-decoder-residual-bp/",
    "publishdate": "2013-12-13T22:53:25-04:00",
    "relpermalink": "/project/ldpc-decoder-residual-bp/",
    "section": "project",
    "summary": "A study of the effects of residual belief propagation as applied to LDPC decoding",
    "tags": [
      "signal-processing"
    ],
    "title": "LDPC decoding using residual BP",
    "type": "project"
  },
  {
    "authors": null,
    "categories": null,
    "content": "Because I can publish shitty drawings, and even shittier text.\nAnd I have no obligation to be accurate.\n\n",
    "date": 1377985597,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1377985597,
    "objectID": "89398b61d8d2dd66a3d59be61a76d973",
    "permalink": "https://skandhurkat.com/post/history-of-computing/",
    "publishdate": "2013-08-31T17:46:37-04:00",
    "relpermalink": "/post/history-of-computing/",
    "section": "post",
    "summary": "Because I can publish shitty drawings, and even shittier text.\nAnd I have no obligation to be accurate.\n\n",
    "tags": [
      "personal-opinions",
      "fun"
    ],
    "title": "A history of computing -- my perspective",
    "type": "post"
  },
  {
    "authors": [
      "Skand Hurkat",
      "Xiaodong Wang"
    ],
    "categories": null,
    "content": "Belief propagation on probabilistic graphical models such as Markov random fields is the basis for a variety of applications, especially applications in computer vision. In this project, we used Vivado-HLS, a C-to-gates framework to rapidly prototype and test a number of accelerators for belief propagation on an FPGA. We used a Xilinx Zynq platform and our experiments show that using a dedicated FPGA accelerator provides a 2x speedup compared to a processor-only approach. Further, we note that the speedup is limited by communication bandwidth, and that higher speedups could be achieved using a higher-bandwidth bus.\n",
    "date": 1368844933,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1368844933,
    "objectID": "b63b54d73e874a29eb30bb7bf27a6ff3",
    "permalink": "https://skandhurkat.com/project/hls-bp/",
    "publishdate": "2013-05-17T22:42:13-04:00",
    "relpermalink": "/project/hls-bp/",
    "section": "project",
    "summary": "Using Vivado-HLS to rapidly prototype and test accelerators",
    "tags": [
      "computer-vision",
      "hardware"
    ],
    "title": "High-level synthesis of a belief propagation accelerator",
    "type": "project"
  },
  {
    "authors": null,
    "categories": null,
    "content": "I use Linux when I work from home, I\u0026rsquo;m forced to use a Mac at work (well, I boot up a virtual Linux OS), and I use Windows when I just want to goof around with my computer. So, while most of my work is done on Linux, it\u0026rsquo;s imperative that my code work on all platforms; just because I could use any of the three.\nTraditionally, C required multiple versions of code, protected by #ifdefs. This often required multiple versions of code to be written, depending on the target system, target OS, and compiler being used. Clumsy and messy system.\nC++ too had similar shortcomings. When it came to writing multi-threaded code, I had to choose either Win32 or Posix, and once I made that choice, I was bound by it. Since those were the days when Ubuntu was driving me crazy, I chose Win32. Bad decision.\nEvery single action that I attempted was compounded by the fact that Win32 is the worst API ever. How do I lock a mutex? Well, first I declare a handle, then declare a mutex, then define the handle to point to the mutex, then attempt to lock the mutex, specifying a timeout interval, then check to see if the error on the acquisition is ERROR_SUCCESS. A crazy system which leads to crazy code.\nAnd that\u0026rsquo;s not compatible with Posix, which is a much cleaner API.\nSo, when C++11 was announced, I jumped with joy at the fact that multi-threading support was built into the language, and that the proposed interface was so much similar to the cleaner Posix API. C++11 allows me to get rid of the system dependent multi-threading APIs, and focus on the code at hand that actually solves the problem. Not only that, C++11 specifies memory models for atomic operations; which allows me to atomically load, store and swap values. The only way this would be possible prior to C++11 was to declare a mutex for every atomic operation. Not a good idea, as it would lead to a tonne of mutexes, with large, irrelevant scope.\nThe other, messier option would be to dive down into the assembly level of the target platform, and write in some assembly to atomically load or store some values.\nCombine the improved multi-threading and memory models in C++11 with CMake, and I get a nice cross platform code, which works on multiple platforms; well, almost. To be really sure, I need to test the code on each platform; but it\u0026rsquo;s relatively harder to mess up, the most chances occur in CMake, where I need to define compiler options for different build environments using a number of conditional statements. It\u0026rsquo;s worse, because CMake is scripted, and that means that there may be conditions which are written syntactically incorrect, but I would not be aware of this until I actually tried to build on a system that leads to those conditions. Still, CMake does not, or should not make a bulk of the code.\nThe usefulness of writing platform independent code was apparent when I worked on EmoDetect. I used Linux (Ubuntu 12.04), Abhinandan used Mac and Ubuntu (12.10), and both Rishabh and Aayush used Windows (different versions here). Yet, we could collaborate perfectly (again, almost; the three of them had an inexplicable aversion to git, so we ended up passing files (not just patches (OMG!))).\nI\u0026rsquo;d say that it\u0026rsquo;s so important to write platform independent code. I\u0026rsquo;ve been trying (unsuccessfully) to port Darktable to Windows; and while I\u0026rsquo;m sure that it would not be too much effort to port the actual DT code, I\u0026rsquo;m stuck with compiling libraries, all of which were written for GNU, and Windows support was added later as a hack. Some of them don\u0026rsquo;t compile, many need to be fixed, and that\u0026rsquo;s holding up the process indefinitely.\nThat and the fact that I now almost always use Ubuntu, which means that I don\u0026rsquo;t really bother about Windows software any longer.\n",
    "date": 1363469195,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1363469195,
    "objectID": "8f5ac0c7e74367649e908d94a4922358",
    "permalink": "https://skandhurkat.com/post/why-i-like-cpp11/",
    "publishdate": "2013-03-16T17:26:35-04:00",
    "relpermalink": "/post/why-i-like-cpp11/",
    "section": "post",
    "summary": "I use Linux when I work from home, I\u0026rsquo;m forced to use a Mac at work (well, I boot up a virtual Linux OS), and I use Windows when I just want to goof around with my computer. So, while most of my work is done on Linux, it\u0026rsquo;s imperative that my code work on all platforms; just because I could use any of the three.\nTraditionally, C required multiple versions of code, protected by #ifdefs.",
    "tags": [
      "programming",
      "c++",
      "cmake"
    ],
    "title": "On writing platform-independent code (or why I like the new C++)",
    "type": "post"
  },
  {
    "authors": [
      "Rishabh Animesh",
      "Skand Hurkat",
      "Abhinandan Majumdar",
      "Aayush Saxena"
    ],
    "categories": null,
    "content": "We developed EmoDetect, an open source system for identifying human emotion from images. This system has an accuracy of around 63% in identifying the correct emotion from amongst seven candidates. Our experiments showed that humans did slightly better, with an accuracy of 74%.\nThe project was aimed as a comparative study of different feature extractors and learning algorithm combinations for the task of identifying emotion from static images. Our study revealed that Gabor features with a linear, soft margin SVM performed best.\n",
    "date": 1353095543,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1353095543,
    "objectID": "095ec5f6fe37275c3e2249a27e1321dc",
    "permalink": "https://skandhurkat.com/project/emodetect/",
    "publishdate": "2012-11-16T15:52:23-04:00",
    "relpermalink": "/project/emodetect/",
    "section": "project",
    "summary": "Emotion detection from images",
    "tags": [
      "computer-vision",
      "machine-learning"
    ],
    "title": "EmoDetect",
    "type": "project"
  },
  {
    "authors": null,
    "categories": null,
    "content": "People familiar with the GNU/Unix system would know that the standard way to install about any GNU software from code is to run the following commands\nconfigure make sudo make install  These commands are from the GNU autotools environment. Knowing the GNU environment, these tools are what I used to compile most of the (relatively small) pieces of code I wrote.\nUntil I discovered CMake.\nCMake, or Cross-platform Make is a tool that serves the same functionality as the GNU autotools (hereafter referred to as autohell). As the name suggests, CMake can be used to compile software across multiple platforms.\nWhy am I encouraging CMake over autohell? Well, firstly, it‚Äôs not autohell! The GNU autotools are extremely useful when compiling on Linux. That‚Äôs what they are designed for. Cross compilation is extremely easy with the --host= flag. Unfortunately, when it comes to using them on Windows, the system just becomes a pain in the neck.\nPeople who have tried to compile a GNU package on Windows would know what I‚Äôm speaking about. You type in configure, wait for about half an hour while the script does its job in the MSYS terminal, then enter make -j4 (to speed up the process), wait for an hour while libtool compiles the bloody code, then make install (finally!).\nThe reason why the process is so painfully slow is that autotools, at their heart are just shell scripts. This means that while compiling for Windows, in an MSYS terminal, the shell script is parsed by sh.exe, which is a slow emulator of the Unix shell. Further, when the configure script has done its job, the Makefile takes over, which relegates all compile operations to libtool, which is another bloody shell script. More sh.exe calls! The result? A painfully slow compile and installation process. Contrast that with CMake. Now, CMake exists on all platforms. It‚Äôs simple and elegant, and allows a choice of compilers, right from MSYS Makefiles, to CodeBlocks MinGW projects, to Visual Studio project files.\nTo prove my point, I‚Äôd just recommend that you checkout a simple project (Library management system, my 12th grade project) in my GitHub repository, which can be compiled using autohell as well as CMake. The CMake step takes a second, while autohell takes over a minute (on Windows). Moreover, the CMakeLists.txt file is short and sweet, and was written in under a minute. By contrast, I took ages to write autohell files, what with configure.ac, Makefile.am and stuff. I switched from autohell to CMake immediately.\n",
    "date": 1345152258,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1345152258,
    "objectID": "ed19da7cadf1b92833445c925a0bda67",
    "permalink": "https://skandhurkat.com/post/autohell-v-cmake/",
    "publishdate": "2012-08-16T17:24:18-04:00",
    "relpermalink": "/post/autohell-v-cmake/",
    "section": "post",
    "summary": "People familiar with the GNU/Unix system would know that the standard way to install about any GNU software from code is to run the following commands\nconfigure make sudo make install  These commands are from the GNU autotools environment. Knowing the GNU environment, these tools are what I used to compile most of the (relatively small) pieces of code I wrote.\nUntil I discovered CMake.\nCMake, or Cross-platform Make is a tool that serves the same functionality as the GNU autotools (hereafter referred to as autohell).",
    "tags": [
      "programming",
      "cmake"
    ],
    "title": "AutoHell v CMake",
    "type": "post"
  },
  {
    "authors": [
      "Skand Hurkat"
    ],
    "categories": null,
    "content": "This project considers the problem of delay-constrained scheduling over wireless fading channels in OFDMA networks like LTE-Advanced networks. Existing scheduling algorithms are considered and extended to OFDMA networks, and performance is evaluated. Specifically, the problem of scheduling users on the downlink in TD-LTE networks has been addressed, and suitably modified proportional-fair and opportunistic schedulers are proposed. Their performance is evaluated in the context of downlink in TD-LTE systems, and compared. Further, a simulation environment has been created which can be used for further analysis of scheduling algorithms in TD-LTE networks, and which can be suitably extended for simulating relay-assisted networks.\n",
    "date": 1335818450,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1335818450,
    "objectID": "8ef556cdab88c736ba70c14b292d2b23",
    "permalink": "https://skandhurkat.com/project/btp/",
    "publishdate": "2012-04-30T16:40:50-04:00",
    "relpermalink": "/project/btp/",
    "section": "project",
    "summary": "BTech project, IIT Bombay",
    "tags": [
      "signal-processing"
    ],
    "title": "Delay-aware proportional-fair scheduling in OFDMA networks",
    "type": "project"
  },
  {
    "authors": [],
    "categories": null,
    "content": "I worked on this project as a summer intern at √âcole de technologie sup√©rieure, Montr√©al, Canada, under the guidance of Dr. Michael McGuffin. My work involved developing new and innovative gestures for network layout manipulation. Over the course of almost 10 weeks, I converted an existing interface from a mouse-keyboard interaction technique to a touch based technique, with special gestures like radial menus, toolglass widgets and popup menus to enable easy and intuitive interaction with networks.\n",
    "date": 1310849114,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1310849114,
    "objectID": "4c121daeb3cc4a4c09369bdd9fa49cdb",
    "permalink": "https://skandhurkat.com/project/hci/",
    "publishdate": "2011-07-16T16:45:14-04:00",
    "relpermalink": "/project/hci/",
    "section": "project",
    "summary": "I worked on this project as a summer intern at √âcole de technologie sup√©rieure, Montr√©al, Canada, under the guidance of Dr. Michael McGuffin. My work involved developing new and innovative gestures for network layout manipulation. Over the course of almost 10 weeks, I converted an existing interface from a mouse-keyboard interaction technique to a touch based technique, with special gestures like radial menus, toolglass widgets and popup menus to enable easy and intuitive interaction with networks.",
    "tags": [
      "human-computer-interaction"
    ],
    "title": "Multi-touch interface for visualising network information",
    "type": "project"
  },
  {
    "authors": [
      "Skand Hurkat",
      "Indrasen Bhattacharya",
      "Srujan Meesala"
    ],
    "categories": null,
    "content": "This was a semester long team course project as part of the Electronics Design Lab (the dreaded \u0026ldquo;EDL\u0026rdquo;) course at IITB. The aim of this project was to build an affordable pair of noise cancelling headphones.\nWe developed an analogue feedback based controller as a disturbance rejection control system; which had the added benefit of improving the sound quality of our relatively cheap $10 headphones. The main challenges in this project were to characterise the headphone-ear system (the \u0026ldquo;plant\u0026rdquo;) to get the frequency magnitude and phase response; as the system changes characteristics based on a number of factors, including the shape of the ear cavity, the way the headphones are worn, and the placement of the microphones inside the headphone cavity.\nOur control system resulted in a successful attenuation of noise, with a peak cancellation of 17 db at 100 Hz. We aimed for maximum cancellation at 100 Hz, as that\u0026rsquo;s the frequency of noise from appliances in an office setting, like fans and airconditioning; when the AC mains supplies power at 50 Hz.\nOur system was designed to work at 3 V, that is two dry cells. However, we did not optimise the power consumption to maximise battery life, given the limited time on the project; and because that would mean micro-optimisation before macro-optimisation.\n",
    "date": 1304455644,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1304455644,
    "objectID": "003c49f95eec8a23923623e42ef5e1fc",
    "permalink": "https://skandhurkat.com/project/edl/",
    "publishdate": "2011-05-03T16:47:24-04:00",
    "relpermalink": "/project/edl/",
    "section": "project",
    "summary": "Low-cost analogue active noise cancelling headphones",
    "tags": [
      "signal-processing",
      "hardware"
    ],
    "title": "Active noise-cancelling headphones",
    "type": "project"
  },
  {
    "authors": [
      "Skand Hurkat"
    ],
    "categories": null,
    "content": "CarDetect is a system to identify car numberplates (registration plates) and car make and model. This system was supposed to help law enforcement officials identify fake numberplates and stolen cars. I worked on this project for over a year and achieved over 98% accuracy in recognizing numberplate regions, and over 90% accuracy in recognizing car models.\nThe project was implemented in OpenCV using C/C++. The code for the project can be found on my GitHub repository.\nPlease note that I\u0026rsquo;m no longer working on CarDetect. The code on my GitHub repository has not been changed since 2011, with the exception of adding a build system using CMake. I\u0026rsquo;ve also modified the code to use C++11 features; including the multithreaded model, but performance is not as good as the original Windows code, found in the Woe32 branch.\n",
    "date": 1302986256,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1302986256,
    "objectID": "bd439fa2b55ef11bdf8fb75aae84ad45",
    "permalink": "https://skandhurkat.com/project/cardetect/",
    "publishdate": "2011-04-16T16:37:36-04:00",
    "relpermalink": "/project/cardetect/",
    "section": "project",
    "summary": "Car make and model detection",
    "tags": [
      "machine-learning",
      "computer-vision"
    ],
    "title": "CarDetect",
    "type": "project"
  },
  {
    "authors": [
      "Skand Hurkat",
      "Yogesh Kumbhejkar"
    ],
    "categories": null,
    "content": "We developed a system to identify users independent of their spoken words. The system used Mel-frequency cepstral coefficients (MFCCs) to obtain a characteristic pattern of the user\u0026rsquo;s voice which was then analysed by a three layer multi-layer perceptron (MLP) neural network. We obtained accuracies of over 90% using this approach.\nThe entire project was implemented in MATLAB.\n",
    "date": 1271452155,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1271452155,
    "objectID": "a52846a48f70c61040a52e2f16c80e86",
    "permalink": "https://skandhurkat.com/project/text-independent-speaker-verification/",
    "publishdate": "2010-04-16T17:09:15-04:00",
    "relpermalink": "/project/text-independent-speaker-verification/",
    "section": "project",
    "summary": "Artificial neural networks to identify users from speech",
    "tags": [
      "signal-processing",
      "machine-learning"
    ],
    "title": "Text-independent speaker verification",
    "type": "project"
  },
  {
    "authors": [
      "Skand Hurkat",
      "Pritish Kamath",
      "Aditya Mehta"
    ],
    "categories": null,
    "content": "This was a summer project in my first year. This system used an ATmega128 microcontroller coupled with a 62256 SRAM module. It could store and verify a 2 second speech signal by any one speaker. Support for additional speakers may be added by adding an interface to an SD card, and tweaking the code slightly.\nThe system was designed to be compatible with a computer microphone for accepting the input signal.\n",
    "date": 1247778911,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1247778911,
    "objectID": "8fa46b0ccd995f974f6e8f104f3d3813",
    "permalink": "https://skandhurkat.com/project/text-dependent-speaker-verification/",
    "publishdate": "2009-07-16T17:15:11-04:00",
    "relpermalink": "/project/text-dependent-speaker-verification/",
    "section": "project",
    "summary": "Matching spoken text to identify users on an Atmel AVR microcontroller",
    "tags": [
      "signal-processing"
    ],
    "title": "Text-dependent speaker verification",
    "type": "project"
  }
]